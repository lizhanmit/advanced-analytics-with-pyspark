{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9d53722-b980-4177-aacd-7407689cb963",
   "metadata": {},
   "source": [
    "# Chapter 2. Introduction to Data Analysis with PySpark\n",
    "\n",
    "The dataset used in below code was curated from a record linkage study performed at a German hospital in 2010, and it contains several million pairs of patient records that were matched according to several different criteria, such as the patientâ€™s name (first and last), address, and birthday. Each matching field was assigned a numerical score from 0.0 to 1.0 based on how similar the strings were, and the data was then hand-labeled to identify which pairs represented the same person and which did not.\n",
    "\n",
    "The first two fields are integer IDs that represent the patients who were matched in the record.\n",
    "\n",
    "The following fields are stored as integers when the only possible values are match (1) or no-match (0), and doubles whenever partial matches are possible.\n",
    "\n",
    "The last field is a boolean value (true or false) indicating whether or not the pair of patient records represented by the line was a match.\n",
    "\n",
    "The goal is to come up with a simple classifier that allows us to predict whether a record will be a match based on the values of the match scores for the patient records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af5408f2-3260-4caa-9d6b-a861da2665f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "482af1a3-cde9-4aa3-965b-b3f61e1e698a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/24 08:17:14 WARN Utils: Your hostname, green-nbjupyterhub12 resolves to a loopback address: 127.0.0.1; using 10.0.0.73 instead (on interface ens5)\n",
      "24/12/24 08:17:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/24 08:17:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.config(\"spark.driver.memory\", \"4g\").appName(\"intro_to_pyspark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4533cf-3392-4671-a05e-4385fcc29016",
   "metadata": {},
   "source": [
    "When Spark reads a CSV file without schema inference, by default, every column in it is treated as a `string` type.\n",
    "\n",
    "In order to perform the schema inference, Spark must do two passes over the dataset:\n",
    "one pass to figure out the type of each column, and a second pass to do the actual\n",
    "parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "568fb8a8-f128-4ea8-a0ea-54fab447209e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "parsed_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"nullValue\", \"?\").csv(\"./datasets/donation/block_*.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cf53b7-e802-41fa-be58-415d1b2d968b",
   "metadata": {},
   "source": [
    "## Analyzing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2f3c154-4fe5-4635-ac0c-9554aa61a5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_1: integer (nullable = true)\n",
      " |-- id_2: integer (nullable = true)\n",
      " |-- cmp_fname_c1: double (nullable = true)\n",
      " |-- cmp_fname_c2: double (nullable = true)\n",
      " |-- cmp_lname_c1: double (nullable = true)\n",
      " |-- cmp_lname_c2: double (nullable = true)\n",
      " |-- cmp_sex: integer (nullable = true)\n",
      " |-- cmp_bd: integer (nullable = true)\n",
      " |-- cmp_bm: integer (nullable = true)\n",
      " |-- cmp_by: integer (nullable = true)\n",
      " |-- cmp_plz: integer (nullable = true)\n",
      " |-- is_match: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ca8b64-20dc-497d-82b9-6ffb0c30e83c",
   "metadata": {},
   "source": [
    "If you know the schema that you want to use for a CSV file ahead of time, you can create an instance of the `pyspark.sql.types.StructType` class and pass it to the Reader API via the `schema` function. **This can have a significant performance benefit when the dataset is very large, since Spark will not need to perform an extra pass over the data to figure out the data type of each column.**\n",
    "\n",
    "For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88ca288-50a9-4764-ac15-30b5c4ddfe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"id_1\", IntegerType(), False), \n",
    "        StructField(\"id_2\", StringType(), False),\n",
    "        StructField(\"cmp_fname_c1\", DoubleType(), False)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# or use DDL (Data Definition Language)\n",
    "# schema = \"id_1 INT, id_2 INT, cmp_fname_c1 DOUBLE\"\n",
    "\n",
    "spark.read.schema(schema).csv(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f22118c8-91e0-4db0-b091-80f96237dc49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id_1: int, id_2: int, cmp_fname_c1: double, cmp_fname_c2: double, cmp_lname_c1: double, cmp_lname_c2: double, cmp_sex: int, cmp_bd: int, cmp_bm: int, cmp_by: int, cmp_plz: int, is_match: boolean]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d22549cd-370b-4b6c-9090-e5a59e8591a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:===============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5749132"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "parsed_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4fdd9e10-cd9a-44e9-8162-979c5e52b852",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0639e4be-60d2-430f-9801-670bcc6b7973",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_df.groupBy(\"is_match\").count().orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17e57c97-10ac-41f2-8575-99bb259bfa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba01054-bc3f-4bd3-9af3-29b694088bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default Spark computes the sample standard deviation; there is also a stddev_pop function for computing the population standard deviation.\n",
    "parsed_df.agg(avg(\"cmp_sex\"), stddev(\"cmp_sex\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c02b010-c10f-4319-9de1-5ee0e45d7677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to use Spark SQL\n",
    "parsed_df.createOrReplaceTempView(\"linkage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf36358-46eb-4d37-b26f-e9e21f9d1418",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT is_match, COUNT(*) cnt\n",
    "FROM linkage\n",
    "GROUP BY is_match\n",
    "ORDER BY cnt DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0f4297-7f73-4808-849d-0a096199fe2f",
   "metadata": {},
   "source": [
    "## Fast Summary Statistics for DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af2abc72-e0c6-4e6d-b990-38ffd60f2eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/24 08:19:34 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 6:===============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|summary|              id_1|              id_2|      cmp_fname_c1|      cmp_fname_c2|      cmp_lname_c1|       cmp_lname_c2|           cmp_sex|             cmp_bd|             cmp_bm|             cmp_by|            cmp_plz|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|  count|           5749132|           5749132|           5748125|            103698|           5749132|               2464|           5749132|            5748337|            5748337|            5748337|            5736289|\n",
      "|   mean| 33324.48559643438| 66587.43558331935|0.7129024704436274|0.9000176718903216|0.3156278193084133|0.31841283153174377| 0.955001381078048|0.22446526708507172|0.48885529849763504| 0.2227485966810923|0.00552866147434343|\n",
      "| stddev|23659.859374488213|23620.487613269885|0.3887583596162788|0.2713176105782331|0.3342336339615816|0.36856706620066537|0.2073011111689795| 0.4172297223846255| 0.4998758236779038|0.41609096298317344|0.07414914925420066|\n",
      "|    min|                 1|                 6|               0.0|               0.0|               0.0|                0.0|                 0|                  0|                  0|                  0|                  0|\n",
      "|    max|             99980|            100000|               1.0|               1.0|               1.0|                1.0|                 1|                  1|                  1|                  1|                  1|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "summary_df = parsed_df.describe()\n",
    "summary_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d7637296-4c5f-486d-bb77-ad7506afd4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_df = parsed_df.where(\"is_match = true\")  # SQL-style syntax\n",
    "match_summary_df = matches_df.describe()\n",
    "\n",
    "misses_df = parsed_df.filter(col(\"is_match\") == False)  # DataFrame API syntax\n",
    "miss_summary_df = misses_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ccad3e-b6fd-43a0-b0d5-b68b5e30cf24",
   "metadata": {},
   "source": [
    "## Pivoting DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823b51ca-1d8c-4cb4-868b-e6c549c361f9",
   "metadata": {},
   "source": [
    "PySpark allows conversion between Spark and pandas DataFrames because of the Apache Arrow project, which allows efficient data transfer between JVM and Python processes.\n",
    "\n",
    "The PyArrow library is installed as a dependency of the Spark SQL module when pyspark is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf28920d-0a56-4f2d-9909-2551580a3695",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "summary_p_df = summary_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f762333-522d-4c26-ab79-93bd1326fbe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>id_1</th>\n",
       "      <th>id_2</th>\n",
       "      <th>cmp_fname_c1</th>\n",
       "      <th>cmp_fname_c2</th>\n",
       "      <th>cmp_lname_c1</th>\n",
       "      <th>cmp_lname_c2</th>\n",
       "      <th>cmp_sex</th>\n",
       "      <th>cmp_bd</th>\n",
       "      <th>cmp_bm</th>\n",
       "      <th>cmp_by</th>\n",
       "      <th>cmp_plz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>5749132</td>\n",
       "      <td>5749132</td>\n",
       "      <td>5748125</td>\n",
       "      <td>103698</td>\n",
       "      <td>5749132</td>\n",
       "      <td>2464</td>\n",
       "      <td>5749132</td>\n",
       "      <td>5748337</td>\n",
       "      <td>5748337</td>\n",
       "      <td>5748337</td>\n",
       "      <td>5736289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>33324.48559643438</td>\n",
       "      <td>66587.43558331935</td>\n",
       "      <td>0.7129024704436274</td>\n",
       "      <td>0.9000176718903216</td>\n",
       "      <td>0.3156278193084133</td>\n",
       "      <td>0.31841283153174377</td>\n",
       "      <td>0.955001381078048</td>\n",
       "      <td>0.22446526708507172</td>\n",
       "      <td>0.48885529849763504</td>\n",
       "      <td>0.2227485966810923</td>\n",
       "      <td>0.00552866147434343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>23659.859374488213</td>\n",
       "      <td>23620.487613269885</td>\n",
       "      <td>0.3887583596162788</td>\n",
       "      <td>0.2713176105782331</td>\n",
       "      <td>0.3342336339615816</td>\n",
       "      <td>0.36856706620066537</td>\n",
       "      <td>0.2073011111689795</td>\n",
       "      <td>0.4172297223846255</td>\n",
       "      <td>0.4998758236779038</td>\n",
       "      <td>0.41609096298317344</td>\n",
       "      <td>0.07414914925420066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>99980</td>\n",
       "      <td>100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary                id_1                id_2        cmp_fname_c1  \\\n",
       "0   count             5749132             5749132             5748125   \n",
       "1    mean   33324.48559643438   66587.43558331935  0.7129024704436274   \n",
       "2  stddev  23659.859374488213  23620.487613269885  0.3887583596162788   \n",
       "3     min                   1                   6                 0.0   \n",
       "4     max               99980              100000                 1.0   \n",
       "\n",
       "         cmp_fname_c2        cmp_lname_c1         cmp_lname_c2  \\\n",
       "0              103698             5749132                 2464   \n",
       "1  0.9000176718903216  0.3156278193084133  0.31841283153174377   \n",
       "2  0.2713176105782331  0.3342336339615816  0.36856706620066537   \n",
       "3                 0.0                 0.0                  0.0   \n",
       "4                 1.0                 1.0                  1.0   \n",
       "\n",
       "              cmp_sex               cmp_bd               cmp_bm  \\\n",
       "0             5749132              5748337              5748337   \n",
       "1   0.955001381078048  0.22446526708507172  0.48885529849763504   \n",
       "2  0.2073011111689795   0.4172297223846255   0.4998758236779038   \n",
       "3                   0                    0                    0   \n",
       "4                   1                    1                    1   \n",
       "\n",
       "                cmp_by              cmp_plz  \n",
       "0              5748337              5736289  \n",
       "1   0.2227485966810923  0.00552866147434343  \n",
       "2  0.41609096298317344  0.07414914925420066  \n",
       "3                    0                    0  \n",
       "4                    1                    1  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_p_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d16664fc-450d-4738-bd2b-7398993b231c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 12)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_p_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0cbf100d-369b-45ca-8348-37dc1de3cddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_p_df = summary_p_df.set_index(\"summary\").transpose().reset_index() \\\n",
    ".rename(columns = {\"index\": \"field\"}) \\\n",
    ".rename_axis(None, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3494cd00-d7f7-4071-ae92-fdde87462b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>stddev</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_1</td>\n",
       "      <td>5749132</td>\n",
       "      <td>33324.48559643438</td>\n",
       "      <td>23659.859374488213</td>\n",
       "      <td>1</td>\n",
       "      <td>99980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_2</td>\n",
       "      <td>5749132</td>\n",
       "      <td>66587.43558331935</td>\n",
       "      <td>23620.487613269885</td>\n",
       "      <td>6</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cmp_fname_c1</td>\n",
       "      <td>5748125</td>\n",
       "      <td>0.7129024704436274</td>\n",
       "      <td>0.3887583596162788</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cmp_fname_c2</td>\n",
       "      <td>103698</td>\n",
       "      <td>0.9000176718903216</td>\n",
       "      <td>0.2713176105782331</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cmp_lname_c1</td>\n",
       "      <td>5749132</td>\n",
       "      <td>0.3156278193084133</td>\n",
       "      <td>0.3342336339615816</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cmp_lname_c2</td>\n",
       "      <td>2464</td>\n",
       "      <td>0.31841283153174377</td>\n",
       "      <td>0.36856706620066537</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cmp_sex</td>\n",
       "      <td>5749132</td>\n",
       "      <td>0.955001381078048</td>\n",
       "      <td>0.2073011111689795</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cmp_bd</td>\n",
       "      <td>5748337</td>\n",
       "      <td>0.22446526708507172</td>\n",
       "      <td>0.4172297223846255</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cmp_bm</td>\n",
       "      <td>5748337</td>\n",
       "      <td>0.48885529849763504</td>\n",
       "      <td>0.4998758236779038</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cmp_by</td>\n",
       "      <td>5748337</td>\n",
       "      <td>0.2227485966810923</td>\n",
       "      <td>0.41609096298317344</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cmp_plz</td>\n",
       "      <td>5736289</td>\n",
       "      <td>0.00552866147434343</td>\n",
       "      <td>0.07414914925420066</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           field    count                 mean               stddev  min  \\\n",
       "0           id_1  5749132    33324.48559643438   23659.859374488213    1   \n",
       "1           id_2  5749132    66587.43558331935   23620.487613269885    6   \n",
       "2   cmp_fname_c1  5748125   0.7129024704436274   0.3887583596162788  0.0   \n",
       "3   cmp_fname_c2   103698   0.9000176718903216   0.2713176105782331  0.0   \n",
       "4   cmp_lname_c1  5749132   0.3156278193084133   0.3342336339615816  0.0   \n",
       "5   cmp_lname_c2     2464  0.31841283153174377  0.36856706620066537  0.0   \n",
       "6        cmp_sex  5749132    0.955001381078048   0.2073011111689795    0   \n",
       "7         cmp_bd  5748337  0.22446526708507172   0.4172297223846255    0   \n",
       "8         cmp_bm  5748337  0.48885529849763504   0.4998758236779038    0   \n",
       "9         cmp_by  5748337   0.2227485966810923  0.41609096298317344    0   \n",
       "10       cmp_plz  5736289  0.00552866147434343  0.07414914925420066    0   \n",
       "\n",
       "       max  \n",
       "0    99980  \n",
       "1   100000  \n",
       "2      1.0  \n",
       "3      1.0  \n",
       "4      1.0  \n",
       "5      1.0  \n",
       "6        1  \n",
       "7        1  \n",
       "8        1  \n",
       "9        1  \n",
       "10       1  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_p_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a42bcb1-1055-4d85-97f5-0dfdb197a4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_t_df = spark.createDataFrame(summary_p_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c2dd73c-a827-4c94-8911-d7eb1d8badda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-------------------+-------------------+---+------+\n",
      "|       field|  count|               mean|             stddev|min|   max|\n",
      "+------------+-------+-------------------+-------------------+---+------+\n",
      "|        id_1|5749132|  33324.48559643438| 23659.859374488213|  1| 99980|\n",
      "|        id_2|5749132|  66587.43558331935| 23620.487613269885|  6|100000|\n",
      "|cmp_fname_c1|5748125| 0.7129024704436274| 0.3887583596162788|0.0|   1.0|\n",
      "|cmp_fname_c2| 103698| 0.9000176718903216| 0.2713176105782331|0.0|   1.0|\n",
      "|cmp_lname_c1|5749132| 0.3156278193084133| 0.3342336339615816|0.0|   1.0|\n",
      "|cmp_lname_c2|   2464|0.31841283153174377|0.36856706620066537|0.0|   1.0|\n",
      "|     cmp_sex|5749132|  0.955001381078048| 0.2073011111689795|  0|     1|\n",
      "|      cmp_bd|5748337|0.22446526708507172| 0.4172297223846255|  0|     1|\n",
      "|      cmp_bm|5748337|0.48885529849763504| 0.4998758236779038|  0|     1|\n",
      "|      cmp_by|5748337| 0.2227485966810923|0.41609096298317344|  0|     1|\n",
      "|     cmp_plz|5736289|0.00552866147434343|0.07414914925420066|  0|     1|\n",
      "+------------+-------+-------------------+-------------------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary_t_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "efce3e85-b838-499c-bc5a-ce76696adea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- field: string (nullable = true)\n",
      " |-- count: string (nullable = true)\n",
      " |-- mean: string (nullable = true)\n",
      " |-- stddev: string (nullable = true)\n",
      " |-- min: string (nullable = true)\n",
      " |-- max: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary_t_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "09ed7780-5bc9-4525-b480-a7fd75249204",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "515ba882-19d4-4520-8102-a3a356be0e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- field: string (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- mean: double (nullable = true)\n",
      " |-- stddev: double (nullable = true)\n",
      " |-- min: double (nullable = true)\n",
      " |-- max: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert data type to double for statistics columns.\n",
    "for c in summary_t_df.columns: \n",
    "    if c == \"field\":\n",
    "        continue\n",
    "    summary_t_df = summary_t_df.withColumn(c, summary_t_df[c].cast(DoubleType()))\n",
    "\n",
    "summary_t_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9e679321-3975-4dda-8bc8-dd39a87ae2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract pivoting logic as a function.\n",
    "def pivot_summary(df):\n",
    "    df_p = df.toPandas()\n",
    "    df_p = df_p.set_index(\"summary\").transpose().reset_index()\n",
    "    df_p = df_p.rename(columns = {\"index\": \"field\"}).rename_axis(None, axis=1)\n",
    "    \n",
    "    df_t = spark.createDataFrame(df_p)\n",
    "\n",
    "    for c in df_t.columns: \n",
    "        if c == \"field\":\n",
    "            continue\n",
    "        df_t = df_t.withColumn(c, df_t[c].cast(DoubleType()))\n",
    "\n",
    "    return df_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "48d1ebcc-3573-493f-b444-f65d9a8f9c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "match_summary_t_df = pivot_summary(match_summary_df)\n",
    "miss_summary_t_df = pivot_summary(miss_summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01909085-779b-4482-af16-ebf4cd2cb19b",
   "metadata": {},
   "source": [
    "## Joining DataFrames and Selecting Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "36cf9767-add0-423e-9d22-6a2de498c8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+--------------------+\n",
      "|       field|    total|               delta|\n",
      "+------------+---------+--------------------+\n",
      "|     cmp_plz|5736289.0|  0.9563812499852176|\n",
      "|cmp_lname_c2|   2464.0|  0.8064147192926264|\n",
      "|      cmp_by|5748337.0|  0.7762059675300512|\n",
      "|      cmp_bd|5748337.0|   0.775442311783404|\n",
      "|cmp_lname_c1|5749132.0|  0.6838772482590526|\n",
      "|      cmp_bm|5748337.0|  0.5109496938298685|\n",
      "|cmp_fname_c1|5748125.0|  0.2854529057460786|\n",
      "|cmp_fname_c2| 103698.0| 0.09104268062280008|\n",
      "|     cmp_sex|5749132.0|0.032408185250332844|\n",
      "+------------+---------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "match_summary_t_df.createOrReplaceTempView(\"match_summary\")\n",
    "miss_summary_t_df.createOrReplaceTempView(\"miss_summary\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT a.field, a.count + b.count total, a.mean - b.mean delta\n",
    "FROM match_summary a INNER JOIN miss_summary b ON a.field = b.field\n",
    "WHERE a.field NOT IN (\"id_1\", \"id_2\")\n",
    "ORDER BY delta DESC, total DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f036b69f-9712-446d-b010-6308787cf4d1",
   "metadata": {},
   "source": [
    "## Scoring and Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eec7c81b-6202-4d00-895e-0bc4d6f58e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cmp_lname_c1 + cmp_plz + cmp_by + cmp_bd + cmp_bm'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_features = [\"cmp_lname_c1\", \"cmp_plz\", \"cmp_by\", \"cmp_bd\", \"cmp_bm\"]\n",
    "\n",
    "sum_expr = \" + \".join(good_features)\n",
    "\n",
    "sum_expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "343a644c-3630-4252-baa8-fb294e727040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "de3e992d-c377-4144-a1b1-cc90835c9dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|score|is_match|\n",
      "+-----+--------+\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  4.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  4.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scored_df = parsed_df.fillna(0, subset = good_features) \\\n",
    ".withColumn(\"score\", expr(sum_expr)) \\\n",
    ".select(\"score\", \"is_match\")\n",
    "\n",
    "scored_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a9a66dfe-77d5-45ba-9e16-111d0a3d7c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# Create contingency table (or cross tabulation or crosstab) \n",
    "# that counts the number of records whose scores fall above/below the threshold value crossed with the number of records \n",
    "# in each of those categories that were/were not matches.\n",
    "def cross_tabs(scored_df: DataFrame, threshold: DoubleType) -> DataFrame: \n",
    "    return scored_df.selectExpr(f\"score >= {threshold} as is_above\", \"is_match\") \\\n",
    "    .groupBy(\"is_above\").pivot(\"is_match\", (\"true\", \"false\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "da978d7d-1bc6-4b75-bdd1-a6c5f32632bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:==============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-------+\n",
      "|is_above| true|  false|\n",
      "+--------+-----+-------+\n",
      "|    true|20871|    637|\n",
      "|   false|   60|5727564|\n",
      "+--------+-----+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cross_tabs(scored_df, 4.0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62427bc4-f902-4dd3-97b0-8b2c09083e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e65f6a7-e0aa-4387-adf4-abb2093b35dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2024.02-py310",
   "language": "python",
   "name": "conda-env-anaconda-2024.02-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
